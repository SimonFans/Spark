-- Spark VS Hadoop

1. Data storage structure:

Spark uses memory to create RDD, store intermediate data in memory, computing in memory.
Hadoop stores intermediate data in disk, HDFS split into multiple blocks.

2. Computing mode:

Spark uses DAG (Direct Acyclic Graph) => Transformation + Action, not only map, reduce method, but use groupby,filter...
Hadoop => Map + Reduce, many IO operations waste much time.

3. Task process:

Spark uses thread, while Hadoop uses process which needs few seconds to start the task.


-- RDD: (Resillient distributed dataset)

Resillient: when dataset size is small, place on one machine, otherwise place on multiple machines.

Distributed: Rdd can be divided into many parts, each part can be on multiple machines.


-- Executor: => a process on worknode

Executor can start (one or multiple) threads to run (one or multiple) tasks 

BlockManager in Executor will use both memory and disk to store data, just in case when there's out of memory, it will store data on disk.


-- Stage:

Job split into multi group tasks, each group task called stage. 


-- Spark component:

one application => one Driver and multiple jobs
one job => multiple stages
one stage => multiple tasks


-- Spark resource management:
1. Yarn
2. Mesos 粗粒分配到容器


-- How to run spark-shell
./bin/spark-shell --master <master-url>

# local
# local[*]
# local[K] : K is the number of threads, one thread does one task
# spark://Host:Port 连接到指定的Spark standalone master, default port is 7077
# yarn-client 已客户端模式连接Yarn集群，集群位置可以在Hadoop_CONF_DIR环境变量中找到, driver在集群之外
# yarn-cluster 已集群模式连接Yarn集群，集群位置可以在Hadoop_CONF_DIR环境变量中找到，driver放到集群某一台机器
# mesos://Host:PORT 连接到指定的Mesos集群，默认端口5050






