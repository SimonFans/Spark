-- Spark VS Hadoop

1. Data storage structure:

Spark uses memory to create RDD, store intermediate data in memory, computing in memory.
Hadoop stores intermediate data in disk, HDFS split into multiple blocks.

2. Computing mode:

Spark uses DAG (Direct Acyclic Graph) => Transformation + Action, not only map, reduce method, but use groupby,filter...
Hadoop => Map + Reduce, many IO operations waste much time.

3. Task process:

Spark uses thread, while Hadoop uses process which needs few seconds to start the task.


-- RDD: (Resillient distributed dataset)

Resillient: when dataset size is small, place on one machine, otherwise place on multiple machines.

Distributed: Rdd can be divided into many parts, each part can be on multiple machines.


-- Executor: => a process on worknode

Executor can start (one or multiple) threads to run (one or multiple) tasks 

BlockManager in Executor will use both memory and disk to store data, just in case when there's out of memory, it will store data on disk.


-- Stage:

Job split into multi group tasks, each group task called stage. 


-- Spark component:

one application => one Driver and multiple jobs
one job => multiple stages
one stage => multiple tasks


-- Spark resource management:
1. Yarn
2. Mesos 粗粒分配到容器


-- How to run spark-shell
./bin/spark-shell --master <master-url> --jars
./bin/spark-shell --help
启动spark-shell后退出 :quit

# local
# local[*]
# local[K] : K is the number of threads, one thread does one task
# spark://Host:Port 连接到指定的Spark standalone master, default port is 7077
# yarn-client 已客户端模式连接Yarn集群，集群位置可以在Hadoop_CONF_DIR环境变量中找到, driver在集群之外
# yarn-cluster 已集群模式连接Yarn集群，集群位置可以在Hadoop_CONF_DIR环境变量中找到，driver放到集群某一台机器
# mesos://Host:PORT 连接到指定的Mesos集群，默认端口5050
# --jars 添加多个JAR包，如果有多个，可以用分号(逗号)隔开。


Repartition VS Coalesce

-- Repartition:
The repartition algorithm does a full shuffle and creates new partitions with data that's distributed evenly.

-- Coalesce:
It avoids a full shuffle. 
COALESCE方法默认情况下不会将分区的数据打乱重新组合，这种情况下的缩减分区可能会导致数据不均衡，出现数据倾斜
如果想要让数据均衡，可以进行shuffle处理，即将第二个参数改为True。但是最后分区内结果会打散，不会保持以前的顺序。
coalesce算子可以扩大分区的，但是如果不进行shuffle操作，是没有意义，不起作用的。
如果想要实现扩大分区的效果，需要使用shuffle操作，即coalesce第二个参数设为true。
spark 提供了一个简化的操作
缩减分区： coalesce, 如果想要数据均衡，可以采用shuffle
扩大分区： repartition, 底层代码调用的就是coalesce,而且肯定采用的shuffle为true
If it's known that the number is decreasing then the executor can safely keep data on the minimum number of partitions, 
only moving the data off the extra nodes, onto the nodes that we kept.

-- GroupBy VS GroupByKey:
1. GroupBy() 需要制定根据哪一个Key去做GroupBy， GroupByKey默认用第一个做为Key。
2. 返回RDD结构：
GroupBy： RDD[(String, Iterable[String, Int])]
GroupByKey: RDD[(String, Iterable[Int])]

-- GroupByKey VS ReduceByKey:
从shuffle的角度:reduceByKey和groupByKey都存在shuffle的操作，
但是reduceByKey可以在shuffle前对分区内相同key的数据进行预聚合(combine)功能，这样会减少落盘的数据量，而groupByKey只是进行分组，不存在数据量减少的问题，reduceByKey性能比较高。
从功能的角度:reduceByKey其实包含分组和聚合的功能。GroupByKey只能分组，不能聚合，所以在分组聚合的场合下，推荐使用reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用groupByKey

** Refer to ShangXueTang Youtube Video



