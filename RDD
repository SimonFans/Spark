-- RDD

1. read-only.
2. you can create a new RDD to make change.

-- RDD method: (map, filter, groupBy, join)

-- RDD implement process:  (Transform & Action), Transform only remember the 轨迹. 

-- RDD 容错性  

1. traditional => 数据复制 或者 记录日志
2. DAG 重新计算丢失分区，无需回滚系统，重算过程在不同节点间并行

-- RDD 特性：
1. 高效容错性
2. 中间结果持久化到内存，数据在内存中的多个RDD操作间进行传递，减免了不必要的读写操作
3. 存放数据都是java对象，避免了不必要的对象序列化和反序列化（转换成字节流读/写入磁盘）

-- RDD narrow dependency:

one father RDD -> one child RDD.    // map, filter
multiple father RDD -> one child RDD  // when join 

-- RDD wide dependency:  <shuffle operation> 如果子分区出现问题，想修复困难，因为要从很多父分区恢复

one father -> multiple child RDD. // groupByKey

-- RDD Stage 的划分
在DAG中进行反向解析，遇到宽依赖就断开，遇到窄依赖就把当前的RDD放入到Stage中，将窄依赖尽量划分到同一个Stage中，实现流水线计算(Mapper/Reducer之间不用互相等)

-- RDD 运行过程
1. 创建RDD对象
2. SparkContext 负责计算RDD之间的依赖关系（分别宽依赖和窄依赖），构建DAG
3. DAG scheduler负责把DAG图分解成多个stage，每个stage中包含多个Task，每个Task会被Task scheduler分发给各个WorkerNode上的executor去执行

-- RDD 创建
1. val lines = sc.textFile("file:///usr/local/spark/word.txt")  本地文件系统
   val lines = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")
   val lines = sc.textFile("/user/hadoop/word.txt")
2. 通过并行集合（数组）创建RDD
   val array = Array(1,2,3,4,5)
   val rdd = sc.parallelize(array) 
   val list = List(1,2,3,4,5)
   val rdd = sc.parallelize(list)
   
-- RDD 编程
1. filter()

val lines = sc.textFile("file///...")
lines.filter(line=>line.contains("Spark")).count()   // count 计算最终RDD里的元素数目

2. map() & reduce()

val lines = sc.textFile("file:///...")
lines.map(line => line.split(" ").size).reduce((a,b)=> if (a>b) a else b)

-- Rdd 持久化
via using persist() 对一个RDD标记为持久化,当遇到第一个行动操作时才会真正的帮你做缓存 

persisit(MEMORY_ONLY)   === cache()
persisit(MEMORY_AND_DISK)
unpersist() 不需要持久化的时候可以删掉







